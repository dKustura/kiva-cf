{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiva collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "# implicit framework\n",
    "import implicit\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.approximate_als import (AnnoyAlternatingLeastSquares, FaissAlternatingLeastSquares,\n",
    "                                      NMSLibAlternatingLeastSquares)\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from implicit.nearest_neighbours import (BM25Recommender, CosineRecommender,\n",
    "                                         TFIDFRecommender, bm25_weight)\n",
    "from implicit.datasets.movielens import get_movielens\n",
    "from implicit.evaluation import precision_at_k, train_test_split\n",
    "\n",
    "# utilities\n",
    "import codecs\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# serialization\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MKL_NUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "%env MKL_NUM_THREADS=1\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_table = pd.read_csv('additional-kiva-snapshot/loans.csv')\n",
    "loans_table = loans_table.sort_values(by='raised_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "funded_loans_table = loans_table[loans_table.status == 'funded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2013-10-01'\n",
    "END_DATE = '2015-05-01'\n",
    "\n",
    "mask = (funded_loans_table['raised_time'] > START_DATE) & (funded_loans_table['raised_time'] <= END_DATE)\n",
    "funded_loans_table = funded_loans_table.loc[mask]\n",
    "del mask\n",
    "\n",
    "funded_loan_ids_set = set(funded_loans_table['loan_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(loans_table, open(\"/pickle/loans_table.p\", \"wb\"))\n",
    "# pickle.dump(funded_loans_table, open(\"pickle/funded_loans_table.p\", \"wb\"))\n",
    "# pickle.dump(funded_loan_ids_set, open(\"pickle/funded_loan_ids_set.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free ram\n",
    "del loans_table\n",
    "del funded_loans_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loans-lenders dict filled\n"
     ]
    }
   ],
   "source": [
    "loans = set()\n",
    "lenders = set()\n",
    "loans_lenders_dict = {}\n",
    "\n",
    "with open('additional-kiva-snapshot/loans_lenders.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    line_num = 0\n",
    "    for row in csv_reader:\n",
    "        if line_num == 0:\n",
    "            line_num += 1\n",
    "            continue\n",
    "        loan_id, lender_ids = row\n",
    "        loan_id = int(loan_id)\n",
    "        if loan_id not in funded_loan_ids_set:\n",
    "            continue\n",
    "        \n",
    "        loans.add(loan_id)\n",
    "        new_lenders = set(lender_ids.split(\", \"))\n",
    "        loans_lenders_dict[loan_id] = new_lenders\n",
    "        lenders.update(new_lenders)\n",
    "        line_num += 1\n",
    "\n",
    "loans = list(loans)\n",
    "lenders = list(lenders)\n",
    "\n",
    "print('Loans-lenders dict filled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders_reverse_index = {k: v for v, k in enumerate(lenders)}\n",
    "utility_matrix = lil_matrix((len(loans), len(lenders)), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled utility matrix\n"
     ]
    }
   ],
   "source": [
    "for loan_index, loan in enumerate(loans):\n",
    "    for lender in loans_lenders_dict[loan]:\n",
    "        lender_index = lenders_reverse_index[lender]\n",
    "        utility_matrix[loan_index, lender_index] = 1.0\n",
    "\n",
    "print('Filled utility matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_matrix = utility_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(utility_matrix, open(\"pickle/utility_matrix.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"als\":  AlternatingLeastSquares,\n",
    "    \"nmslib_als\": NMSLibAlternatingLeastSquares,\n",
    "    \"annoy_als\": AnnoyAlternatingLeastSquares,\n",
    "    \"faiss_als\": FaissAlternatingLeastSquares,\n",
    "    \"tfidf\": TFIDFRecommender,\n",
    "    \"cosine\": CosineRecommender,\n",
    "    \"bpr\": BayesianPersonalizedRanking,\n",
    "    \"bm25\": BM25Recommender\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    print(\"getting model %s\" % model_name)\n",
    "    model_class = MODELS.get(model_name)\n",
    "    if not model_class:\n",
    "        raise ValueError(\"Unknown Model '%s'\" % model_name)\n",
    "\n",
    "    # some default params\n",
    "    if issubclass(model_class, AlternatingLeastSquares):\n",
    "        params = {'factors': 16, 'dtype': np.float32, 'use_gpu': True}\n",
    "    elif model_name == \"bm25\":\n",
    "        params = {'K1': 100, 'B': 0.5}\n",
    "    elif model_name == \"bpr\":\n",
    "        params = {'factors': 63}\n",
    "    else:\n",
    "        params = {}\n",
    "\n",
    "    return model_class(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name=\"als\", alpha=40):    \n",
    "    # create a model from the input data\n",
    "    model = get_model(model_name)\n",
    "    data_matrix = utility_matrix\n",
    "    \n",
    "    # if we're training an ALS based model, weight input by bm25\n",
    "    if issubclass(model.__class__, AlternatingLeastSquares):\n",
    "        # multiply positive inputs with alpha\n",
    "        logging.debug(\"scaling matrix by alpha\")\n",
    "        data_matrix = (data_matrix*alpha).astype('double')\n",
    "        \n",
    "        logging.debug(\"weighting matrix by bm25_weight\")\n",
    "        data_matrix = bm25_weight(data_matrix)\n",
    "\n",
    "        # also disable building approximate recommend index\n",
    "        model.approximate_similar_items = False\n",
    "        \n",
    "    logging.debug(\"training model %s\", model_name)\n",
    "    start = time.time()\n",
    "    model.fit(data_matrix)\n",
    "    logging.debug(\"trained model '%s' in %0.2fs\", model_name, time.time() - start)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recommendations(model, output_filename, N=10):\n",
    "    \"\"\" Generates loan recommendations for each lender in the dataset \"\"\"\n",
    "\n",
    "    # generate recommendations for each lender and write out to a file\n",
    "    start = time.time()\n",
    "    lenders_loans = utility_matrix.T.tocsr()\n",
    "    with tqdm.tqdm(total=len(lenders)) as progress:\n",
    "        with codecs.open(output_filename, \"w\", \"utf8\") as o:\n",
    "            for lender_index, lender in enumerate(lenders):\n",
    "                for loan_index, score in model.recommend(lender_index, lenders_loans, N=N):\n",
    "                    o.write(\"%s\\t%s\\t%s\\n\" % (lender, loans[loan_index], score))\n",
    "                progress.update(1)\n",
    "    logging.debug(\"generated recommendations in %0.2fs\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(alpha=100, model_name=\"als\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_recommendations(model, \"output.tsv\", N=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section is an usage example on the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, ratings = get_movielens('20m')\n",
    "\n",
    "# remove things < min_rating, and convert to implicit dataset\n",
    "# by considering ratings as a binary preference only\n",
    "ratings.data[ratings.data < 4.0] = 0\n",
    "ratings.eliminate_zeros()\n",
    "ratings.data = np.ones(len(ratings.data))\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (bm25_weight(ratings, B=0.9) * 5).tocsr()\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the train/test samples 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(users, items, ratings, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit testing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies, ratings = get_movielens(\"20m\")\n",
    "# train, test = train_test_split(ratings)\n",
    "\n",
    "coo_mat = utility_matrix.tocoo()\n",
    "train, test = train_test_split(coo_mat)\n",
    "\n",
    "model = AlternatingLeastSquares(use_gpu=True)\n",
    "model.fit(train)\n",
    "\n",
    "precision = precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "#map_measure = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision @10: %f' % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc_at_k(model, train_user_items, test_user_items, K=10, show_progress=True):\n",
    "    auc_list = []\n",
    "    lenders_count, loans_count = train_user_items.shape\n",
    "    start = time.time()\n",
    "    \n",
    "    with tqdm.tqdm(total=lenders_count) as progress:\n",
    "        for lender_index in range(lenders_count):\n",
    "            if test_user_items[lender_index, :].nnz == 0:\n",
    "                continue\n",
    "                \n",
    "            lender_row = np.zeros(loans_count)\n",
    "            for loan_index, score in model.recommend(lender_index, train_user_items, N=K):\n",
    "                lender_row[loan_index] = score\n",
    "            \n",
    "            test_lender_row = test_user_items[lender_index, :].toarray().flatten()\n",
    "            roc_auc = roc_auc_score(test_lender_row, lender_row)\n",
    "            auc_list.append(roc_auc)\n",
    "            progress.update(1)\n",
    "            \n",
    "    logging.debug(\"generated mean ROC AUC in %0.2fs\", time.time() - start)\n",
    "    return np.mean(auc_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_prec_auc_at_k(model, train_user_items, test_user_items, K=10, show_progress=True):\n",
    "    auc_list = []\n",
    "    lenders_count, loans_count = train_user_items.shape\n",
    "    start = time.time()\n",
    "    \n",
    "    with tqdm.tqdm(total=lenders_count) as progress:\n",
    "        for lender_index in range(lenders_count):\n",
    "            if test_user_items[lender_index, :].nnz == 0:\n",
    "                continue\n",
    "                \n",
    "            lender_row = np.zeros(loans_count)\n",
    "            for loan_index, score in model.recommend(lender_index, train_user_items, N=K):\n",
    "                lender_row[loan_index] = score\n",
    "            \n",
    "            test_lender_row = test_user_items[lender_index, :].toarray().flatten()\n",
    "            \n",
    "            precision, recall, thresholds = precision_recall_curve(test_lender_row, lender_row, pos_label=1)\n",
    "            prec_auc = auc(recall, precision)                \n",
    "            auc_list.append(prec_auc)\n",
    "            progress.update(1)\n",
    "            \n",
    "    logging.debug(\"generated mean Precision/Recall curve AUC in %0.2fs\", time.time() - start)\n",
    "    return np.mean(auc_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc_at_k2(model, train_user_items, test_user_items, K=10, show_progress=True):\n",
    "    auc_list = []\n",
    "    lenders_count, loans_count = train_user_items.shape\n",
    "    start = time.time()\n",
    "    \n",
    "    with tqdm.tqdm(total=lenders_count) as progress:\n",
    "        for lender_index in range(lenders_count):\n",
    "            if test_user_items[lender_index, :].nnz == 0:\n",
    "                continue\n",
    "                \n",
    "            lender_vect = model.user_factors[lender_index]\n",
    "            liked = train_user_items[lender_index].indices\n",
    "\n",
    "            scores = model.item_factors.dot(lender_vect)\n",
    "            scores = np.delete(scores, liked)\n",
    "            \n",
    "            test_lender_row = test_user_items[lender_index, :].toarray().flatten()\n",
    "            test_lender_row = np.delete(test_lender_row, liked)\n",
    "            \n",
    "            roc_auc = roc_auc_score(test_lender_row, scores)\n",
    "            auc_list.append(roc_auc)\n",
    "            progress.update(1)\n",
    "            \n",
    "    logging.debug(\"generated mean ROC AUC in %0.2fs\", time.time() - start)\n",
    "    return np.mean(auc_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:implicit:GPU training requires factor size to be a multiple of 32. Increasing factors from 100 to 128.\n",
      "DEBUG:implicit:Calculated transpose in 0.187s\n",
      "DEBUG:implicit:Initialized factors in 1.8745572566986084\n",
      "DEBUG:implicit:Running 15 ALS iterations\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 15.0/15 [00:19<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "coo_mat = utility_matrix.tocoo()\n",
    "train, test = train_test_split(coo_mat)\n",
    "train_user_items = train.T.tocsr()\n",
    "test_user_items = test.T.tocsr()\n",
    "#test_user_items = utility_matrix.T.tocsr()\n",
    "\n",
    "model = AlternatingLeastSquares(use_gpu=True)\n",
    "model.fit(train)\n",
    "\n",
    "# mean_prec_auc = mean_prec_auc_at_k(model, train_user_items, test_user_items, K=10)\n",
    "# print('Mean Prec AUC score: ', mean_prec_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean ROC AUC 2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████▌                                                               | 85029/648650 [00:11<01:14, 7533.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-add1ef45eeed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmean_roc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_roc_auc_at_k2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_user_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_user_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean ROC AUC score: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_roc_auc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-c7050c6b41cd>\u001b[0m in \u001b[0;36mmean_roc_auc_at_k2\u001b[1;34m(model, train_user_items, test_user_items, K, show_progress)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlenders_count\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlender_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlenders_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtest_user_items\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlender_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;31m# [i, 1:2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_row_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m             \u001b[1;31m# [i, [1, 2]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0missequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\csr.py\u001b[0m in \u001b[0;36m_get_row_slice\u001b[1;34m(self, i, cslice)\u001b[0m\n\u001b[0;32m    405\u001b[0m             row_indptr, row_indices, row_data = get_csr_submatrix(\n\u001b[0;32m    406\u001b[0m                 \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m                 start, stop)\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;31m# other strides need new code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_roc_auc = mean_roc_auc_at_k2(model, train_user_items, test_user_items, K=10)\n",
    "print('Mean ROC AUC score: ', mean_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies, ratings = get_movielens(\"1m\")\n",
    "# ratings.data[ratings.data < 4.0] = 0\n",
    "# ratings.eliminate_zeros()\n",
    "# ratings.data = np.ones(len(ratings.data))\n",
    "\n",
    "# train, test = train_test_split(ratings)\n",
    "\n",
    "# model = AlternatingLeastSquares(use_gpu=True)\n",
    "# model.fit(train)\n",
    "\n",
    "# # precision = precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "# # map_measure = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "# mean_roc_auc = mean_roc_auc_at_k2(model, train.T.tocsr(), test.T.tocsr(), K=10)\n",
    "# print('Mean ROC AUC score: ', mean_roc_auc)\n",
    "\n",
    "# # mean_prec_auc = mean_prec_auc_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10)\n",
    "# # print('Mean Precision/Recall curve AUC score: ', mean_prec_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START_DATE = '2000-01-01'\n",
    "# END_DATE = '2015-01-01'\n",
    "\n",
    "# mask = (loans_table['raised_time'] > START_DATE) & (loans_table['raised_time'] <= END_DATE)\n",
    "# plot_data = loans_table.loc[mask]\n",
    "\n",
    "plot_data = loans_table\n",
    "\n",
    "plot_data['raised_time'] = pd.to_datetime(plot_data['raised_time'])\n",
    "plot_data['date_month_year'] = plot_data['raised_time'].dt.to_period(\"M\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "g1 = sns.pointplot(x='date_month_year', y='loan_amount', \n",
    "                   data=plot_data)\n",
    "g1.set_xticklabels(g1.get_xticklabels(),rotation=90)\n",
    "g1.set_title(\"Mean Loan by Month Year\", fontsize=15)\n",
    "g1.set_xlabel(\"\")\n",
    "g1.set_ylabel(\"Loan Amount\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas dataset reading\n",
    "###### (used only for data analysis purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders_table = pd.read_csv('additional-kiva-snapshot/lenders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders = lenders_table['permanent_name']\n",
    "lenders = lenders.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_lenders_table = pd.read_csv('additional-kiva-snapshot/loans_lenders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_lenders_table.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_size = utility_matrix.shape[0]*utility_matrix.shape[1] # Number of possible interactions in the matrix\n",
    "num_interactions = len(utility_matrix.nonzero()[0]) # Number of items interacted with\n",
    "sparsity = 100*(1 - (num_interactions/matrix_size))\n",
    "print('Sparsitiy: %f %%' % sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old testing environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/test splitting script is used from [here](https://gist.github.com/tgsmith61591/ce7d614d7a0442f94cd5ae5d1e51d3c2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collab_split import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_mat = utility_matrix.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = coo_mat.row, coo_mat.col, coo_mat.data\n",
    "users = LabelEncoder().fit_transform(users)\n",
    "items = LabelEncoder().fit_transform(items)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
