{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiva collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import implicit\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "# visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import logging\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.approximate_als import (AnnoyAlternatingLeastSquares, FaissAlternatingLeastSquares,\n",
    "                                      NMSLibAlternatingLeastSquares)\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from implicit.nearest_neighbours import (BM25Recommender, CosineRecommender,\n",
    "                                         TFIDFRecommender, bm25_weight)\n",
    "\n",
    "from implicit.datasets.movielens import get_movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MKL_NUM_THREADS=1\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_table = pd.read_csv('additional-kiva-snapshot/loans.csv')\n",
    "loans_table = loans_table.sort_values(by='raised_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funded_loans_table = loans_table[loans_table.status == 'funded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2017-01-01'\n",
    "END_DATE = '2019-01-01'\n",
    "\n",
    "mask = (funded_loans_table['raised_time'] > START_DATE) & (funded_loans_table['raised_time'] <= END_DATE)\n",
    "funded_loans_table = funded_loans_table.loc[mask]\n",
    "\n",
    "funded_loan_ids_set = set(funded_loans_table['loan_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('additional-kiva-snapshot/lenders.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "#     csv_reader = csv.reader(csvfile)\n",
    "#     line_num = 0\n",
    "#     for row in csv_reader:\n",
    "#         if line_num == 0:\n",
    "#             line_num += 1\n",
    "#             continue\n",
    "#         lenders.add(row[0])\n",
    "#         line_num += 1\n",
    "\n",
    "# print('Lenders filled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = set()\n",
    "lenders = set()\n",
    "loans_lenders_dict = {}\n",
    "\n",
    "with open('additional-kiva-snapshot/loans_lenders.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    line_num = 0\n",
    "    for row in csv_reader:\n",
    "        if line_num == 0:\n",
    "            line_num += 1\n",
    "            continue\n",
    "        loan_id, lender_ids = row\n",
    "        loan_id = int(loan_id)\n",
    "        if loan_id not in funded_loan_ids_set:\n",
    "            continue\n",
    "        \n",
    "        loans.add(loan_id)\n",
    "        new_lenders = set(lender_ids.split(\", \"))\n",
    "        loans_lenders_dict[loan_id] = new_lenders\n",
    "        lenders.update(new_lenders)\n",
    "        line_num += 1\n",
    "\n",
    "print('Loans-lenders dict filled')\n",
    "print('Loans set filled')\n",
    "print('Lenders set filled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_list = list(loans)\n",
    "lenders_list = list(lenders)\n",
    "utility_matrix = lil_matrix((len(loans), len(lenders)), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders_reverse_index = {k: v for v, k in enumerate(lenders_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loan_index, loan in enumerate(loans_list):\n",
    "    for lender in loans_lenders_dict[loan]:\n",
    "        lender_index = lenders_reverse_index[lender]\n",
    "        utility_matrix[loan_index, lender_index] = 1\n",
    "    \n",
    "print('Filled utiility matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_matrix = utility_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utility_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas dataset reading\n",
    "###### (used only for data analysis purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders_table = pd.read_csv('additional-kiva-snapshot/lenders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders = lenders_table['permanent_name']\n",
    "lenders = lenders.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_lenders_table = pd.read_csv('additional-kiva-snapshot/loans_lenders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_lenders_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"als\":  AlternatingLeastSquares,\n",
    "    \"nmslib_als\": NMSLibAlternatingLeastSquares,\n",
    "    \"annoy_als\": AnnoyAlternatingLeastSquares,\n",
    "    \"faiss_als\": FaissAlternatingLeastSquares,\n",
    "    \"tfidf\": TFIDFRecommender,\n",
    "    \"cosine\": CosineRecommender,\n",
    "    \"bpr\": BayesianPersonalizedRanking,\n",
    "    \"bm25\": BM25Recommender\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    print(\"getting model %s\" % model_name)\n",
    "    model_class = MODELS.get(model_name)\n",
    "    if not model_class:\n",
    "        raise ValueError(\"Unknown Model '%s'\" % model_name)\n",
    "\n",
    "    # some default params\n",
    "    if issubclass(model_class, AlternatingLeastSquares):\n",
    "        params = {'factors': 16, 'dtype': np.float32, 'use_gpu': True}\n",
    "    elif model_name == \"bm25\":\n",
    "        params = {'K1': 100, 'B': 0.5}\n",
    "    elif model_name == \"bpr\":\n",
    "        params = {'factors': 63}\n",
    "    else:\n",
    "        params = {}\n",
    "\n",
    "    return model_class(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name=\"als\", alpha=40):    \n",
    "    # create a model from the input data\n",
    "    model = get_model(model_name)\n",
    "    data_matrix = utility_matrix\n",
    "    \n",
    "    # if we're training an ALS based model, weight input by bm25\n",
    "    if issubclass(model.__class__, AlternatingLeastSquares):\n",
    "        # multiply positive inputs with alpha\n",
    "        logging.debug(\"scaling matrix by alpha\")\n",
    "        data_matrix = data_matrix.multiply(alpha)\n",
    "        \n",
    "        logging.debug(\"weighting matrix by bm25_weight\")\n",
    "        data_matrix = bm25_weight(data_matrix)\n",
    "\n",
    "        # also disable building approximate recommend index\n",
    "        model.approximate_similar_items = False\n",
    "        \n",
    "    logging.debug(\"training model %s\", model_name)\n",
    "    start = time.time()\n",
    "    model.fit(data_matrix)\n",
    "    logging.debug(\"trained model '%s' in %0.2fs\", model_name, time.time() - start)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recommendations(model, output_filename, N=10):\n",
    "    \"\"\" Generates loan recommendations for each lender in the dataset \"\"\"\n",
    "\n",
    "    # generate recommendations for each lender and write out to a file\n",
    "    start = time.time()\n",
    "    lenders_loans = utility_matrix.T.tocsr()\n",
    "    with tqdm.tqdm(total=len(lenders)) as progress:\n",
    "        with codecs.open(output_filename, \"w\", \"utf8\") as o:\n",
    "            for lender_index, lender in enumerate(lenders_list):\n",
    "                for loan_index, score in model.recommend(lender_index, lenders_loans, N=N):\n",
    "                    o.write(\"%s\\t%s\\t%s\\n\" % (lender, loans_list[loan_index], score))\n",
    "                progress.update(1)\n",
    "    logging.debug(\"generated recommendations in %0.2fs\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(alpha=100, model_name=\"als\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_recommendations(model, \"output.tsv\", N=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section is an usage example on the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, ratings = get_movielens('20m')\n",
    "\n",
    "# remove things < min_rating, and convert to implicit dataset\n",
    "# by considering ratings as a binary preference only\n",
    "ratings.data[ratings.data < 4.0] = 0\n",
    "ratings.eliminate_zeros()\n",
    "ratings.data = np.ones(len(ratings.data))\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (bm25_weight(ratings, B=0.9) * 5).tocsr()\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing environment\n",
    "###### (skip for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/test splitting script is used from [here](https://gist.github.com/tgsmith61591/ce7d614d7a0442f94cd5ae5d1e51d3c2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collab_split import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_mat = utility_matrix.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = coo_mat.row, coo_mat.col, coo_mat.data\n",
    "users = LabelEncoder().fit_transform(users)\n",
    "items = LabelEncoder().fit_transform(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the train/test samples 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(users, items, ratings, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit testing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.evaluation import precision_at_k, train_test_split\n",
    "from implicit.datasets.movielens import get_movielens\n",
    "\n",
    "# movies, ratings = get_movielens(\"20m\")\n",
    "# train, test = train_test_split(ratings)\n",
    "\n",
    "coo_mat = utility_matrix.tocoo()\n",
    "train, test = train_test_split(coo_mat)\n",
    "\n",
    "model = AlternatingLeastSquares(use_gpu=True)\n",
    "model.fit(train)\n",
    "\n",
    "precision = precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "#map_measure = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision @10: %f' % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc_at_k(model, train_user_items, test_user_items, K=10, show_progress=True):\n",
    "    auc_list = []\n",
    "    lenders_count, loans_count = train_user_items.shape\n",
    "    start = time.time()\n",
    "    \n",
    "    with tqdm.tqdm(total=lenders_count) as progress:\n",
    "        for lender_index in range(lenders_count):\n",
    "            lender_row = np.zeros(loans_count)\n",
    "            for loan_index, score in model.recommend(lender_index, train_user_items, N=K):\n",
    "                lender_row[loan_index] = score\n",
    "            \n",
    "            test_lender_row = test_user_items[lender_index, :].toarray().flatten()\n",
    "            if np.count_nonzero(test_lender_row) == 0:\n",
    "#                 print(np.count_nonzero(train_user_items[lender_index, :].toarray().flatten()))\n",
    "#                 print(\"BAD ROW\")\n",
    "                continue\n",
    "            \n",
    "            roc_auc = roc_auc_score(test_lender_row, lender_row)\n",
    "            auc_list.append(roc_auc)\n",
    "            progress.update(1)\n",
    "            \n",
    "    logging.debug(\"generated mean ROC AUC in %0.2fs\", time.time() - start)\n",
    "    return np.mean(auc_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_prec_auc_at_k(model, train_user_items, test_user_items, K=10, show_progress=True):\n",
    "    auc_list = []\n",
    "    lenders_count, loans_count = train_user_items.shape\n",
    "    start = time.time()\n",
    "    \n",
    "    with tqdm.tqdm(total=lenders_count) as progress:\n",
    "        for lender_index in range(lenders_count):\n",
    "            lender_row = np.zeros(loans_count)\n",
    "            for loan_index, score in model.recommend(lender_index, train_user_items, N=K):\n",
    "                lender_row[loan_index] = score\n",
    "            \n",
    "            test_lender_row = test_user_items[lender_index, :].toarray().flatten()\n",
    "            \n",
    "            precision, recall, thresholds = precision_recall_curve(test_lender_row, lender_row, pos_label=1)\n",
    "            prec_auc = auc(recall, precision)                \n",
    "            auc_list.append(prec_auc)\n",
    "            progress.update(1)\n",
    "            \n",
    "    logging.debug(\"generated mean Precision/Recall curve AUC in %0.2fs\", time.time() - start)\n",
    "    return np.mean(auc_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from implicit.evaluation import train_test_split\n",
    "\n",
    "coo_mat = utility_matrix.tocoo()\n",
    "train, test = train_test_split(coo_mat)\n",
    "train_user_items = train.T.tocsr()\n",
    "# test_user_items = test.T.tocsr()\n",
    "test_user_items = utility_matrix.T.tocsr()\n",
    "\n",
    "model = AlternatingLeastSquares(use_gpu=True)\n",
    "model.fit(train)\n",
    "\n",
    "mean_roc_auc = mean_roc_auc_at_k(model, train_user_items, test_user_items, K=10)\n",
    "print('Mean ROC AUC score: ', mean_roc_auc)\n",
    "\n",
    "# mean_prec_auc = mean_prec_auc_at_k(model, train_user_items, test_user_items, K=10)\n",
    "# print('Mean Prec AUC score: ', mean_prec_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.evaluation import precision_at_k, train_test_split\n",
    "from implicit.datasets.movielens import get_movielens\n",
    "\n",
    "movies, ratings = get_movielens(\"1m\")\n",
    "ratings.data[ratings.data < 4.0] = 0\n",
    "ratings.eliminate_zeros()\n",
    "ratings.data = np.ones(len(ratings.data))\n",
    "\n",
    "train, test = train_test_split(ratings)\n",
    "\n",
    "model = AlternatingLeastSquares(use_gpu=True)\n",
    "model.fit(train)\n",
    "\n",
    "# precision = precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "# map_measure = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "mean_roc_auc = mean_roc_auc_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10)\n",
    "print('Mean ROC AUC score: ', mean_roc_auc)\n",
    "\n",
    "# mean_prec_auc = mean_prec_auc_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10)\n",
    "# print('Mean Precision/Recall curve AUC score: ', mean_prec_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START_DATE = '2000-01-01'\n",
    "# END_DATE = '2015-01-01'\n",
    "\n",
    "# mask = (loans_table['raised_time'] > START_DATE) & (loans_table['raised_time'] <= END_DATE)\n",
    "# plot_data = loans_table.loc[mask]\n",
    "\n",
    "plot_data = loans_table\n",
    "\n",
    "plot_data['raised_time'] = pd.to_datetime(plot_data['raised_time'])\n",
    "plot_data['date_month_year'] = plot_data['raised_time'].dt.to_period(\"M\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "g1 = sns.pointplot(x='date_month_year', y='loan_amount', \n",
    "                   data=plot_data)\n",
    "g1.set_xticklabels(g1.get_xticklabels(),rotation=90)\n",
    "g1.set_title(\"Mean Loan by Month Year\", fontsize=15)\n",
    "g1.set_xlabel(\"\")\n",
    "g1.set_ylabel(\"Loan Amount\", fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
