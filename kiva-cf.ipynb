{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiva collaborative filtering\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "# implicit framework\n",
    "import implicit\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.approximate_als import (AnnoyAlternatingLeastSquares, FaissAlternatingLeastSquares,\n",
    "                                      NMSLibAlternatingLeastSquares)\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from implicit.nearest_neighbours import (BM25Recommender, CosineRecommender,\n",
    "                                         TFIDFRecommender, bm25_weight)\n",
    "from implicit.datasets.movielens import get_movielens\n",
    "from implicit.evaluation import precision_at_k, train_test_split\n",
    "\n",
    "# utilities\n",
    "import codecs\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# serialization\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MKL_NUM_THREADS=1\n"
     ]
    }
   ],
   "source": [
    "%env MKL_NUM_THREADS=1\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Pickle loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_table = pickle.load(open(\"pickle/loans_table.p\", \"rb\"))\n",
    "funded_loans_table = pickle.load(open(\"pickle/funded_loans_table.p\", \"rb\"))\n",
    "funded_loan_ids_set = pickle.load(open(\"pickle/funded_loan_ids_set.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_matrix = pickle.load(open(\"pickle/utility_matrix.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_table = pd.read_csv('additional-kiva-snapshot/loans.csv')\n",
    "loans_table = loans_table.sort_values(by='raised_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funded_loans_table = loans_table[loans_table.status == 'funded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START_DATE = '2013-10-01'\n",
    "# END_DATE = '2015-05-01'\n",
    "\n",
    "# dates selected in 'Fairness-Aware Loan Recommendation for Microfinance Services' paper\n",
    "START_DATE = '2011-11-01'\n",
    "END_DATE = '2013-10-30'\n",
    "\n",
    "mask = (funded_loans_table['raised_time'] > START_DATE) & (funded_loans_table['raised_time'] <= END_DATE)\n",
    "funded_loans_table = funded_loans_table.loc[mask]\n",
    "del mask\n",
    "\n",
    "funded_loan_ids_set = set(funded_loans_table['loan_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(loans_table, open(\"pickle/loans_table.p\", \"wb\"))\n",
    "pickle.dump(funded_loans_table, open(\"pickle/funded_loans_table.p\", \"wb\"))\n",
    "pickle.dump(funded_loan_ids_set, open(\"pickle/funded_loan_ids_set.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free ram\n",
    "del loans_table\n",
    "del funded_loans_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = set()\n",
    "lenders = set()\n",
    "loans_lenders_dict = {}\n",
    "\n",
    "with open('additional-kiva-snapshot/loans_lenders.csv', newline='', encoding=\"utf8\") as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    line_num = 0\n",
    "    for row in csv_reader:\n",
    "        if line_num == 0:\n",
    "            line_num += 1\n",
    "            continue\n",
    "        loan_id, lender_ids = row\n",
    "        loan_id = int(loan_id)\n",
    "        if loan_id not in funded_loan_ids_set:\n",
    "            continue\n",
    "        \n",
    "        loans.add(loan_id)\n",
    "        new_lenders = set(lender_ids.split(\", \"))\n",
    "        loans_lenders_dict[loan_id] = new_lenders\n",
    "        lenders.update(new_lenders)\n",
    "        line_num += 1\n",
    "\n",
    "loans = list(loans)\n",
    "lenders = list(lenders)\n",
    "\n",
    "print('Loans-lenders dict filled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Utility matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders_reverse_index = {k: v for v, k in enumerate(lenders)}\n",
    "utility_matrix = lil_matrix((len(loans), len(lenders)), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loan_index, loan in enumerate(loans):\n",
    "    for lender in loans_lenders_dict[loan]:\n",
    "        lender_index = lenders_reverse_index[lender]\n",
    "        utility_matrix[loan_index, lender_index] = 1.0\n",
    "\n",
    "print('Filled utility matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_matrix = utility_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(utility_matrix, open(\"pickle/utility_matrix.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"als\":  AlternatingLeastSquares,\n",
    "    \"nmslib_als\": NMSLibAlternatingLeastSquares,\n",
    "    \"annoy_als\": AnnoyAlternatingLeastSquares,\n",
    "    \"faiss_als\": FaissAlternatingLeastSquares,\n",
    "    \"tfidf\": TFIDFRecommender,\n",
    "    \"cosine\": CosineRecommender,\n",
    "    \"bpr\": BayesianPersonalizedRanking,\n",
    "    \"bm25\": BM25Recommender\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    print(\"getting model %s\" % model_name)\n",
    "    model_class = MODELS.get(model_name)\n",
    "    if not model_class:\n",
    "        raise ValueError(\"Unknown Model '%s'\" % model_name)\n",
    "\n",
    "    # some default params\n",
    "    if issubclass(model_class, AlternatingLeastSquares):\n",
    "        params = {'factors': 16, 'dtype': np.float32, 'use_gpu': True}\n",
    "    elif model_name == \"bm25\":\n",
    "        params = {'K1': 100, 'B': 0.5}\n",
    "    elif model_name == \"bpr\":\n",
    "        params = {'factors': 63}\n",
    "    else:\n",
    "        params = {}\n",
    "\n",
    "    return model_class(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name=\"als\", alpha=40):    \n",
    "    # create a model from the input data\n",
    "    model = get_model(model_name)\n",
    "    data_matrix = utility_matrix\n",
    "    \n",
    "    # if we're training an ALS based model, weight input by bm25\n",
    "    if issubclass(model.__class__, AlternatingLeastSquares):\n",
    "        # multiply positive inputs with alpha\n",
    "        logging.debug(\"scaling matrix by alpha\")\n",
    "        data_matrix = (data_matrix*alpha).astype('double')\n",
    "        \n",
    "#         logging.debug(\"weighting matrix by bm25_weight\")\n",
    "#         data_matrix = bm25_weight(data_matrix)\n",
    "\n",
    "        # also disable building approximate recommend index\n",
    "        model.approximate_similar_items = False\n",
    "        \n",
    "    logging.debug(\"training model %s\", model_name)\n",
    "    start = time.time()\n",
    "    model.fit(data_matrix)\n",
    "    logging.debug(\"trained model '%s' in %0.2fs\", model_name, time.time() - start)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recommendations(model, output_filename, N=10):\n",
    "    \"\"\" Generates loan recommendations for each lender in the dataset \"\"\"\n",
    "\n",
    "    # generate recommendations for each lender and write out to a file\n",
    "    start = time.time()\n",
    "    lenders_loans = utility_matrix.T.tocsr()\n",
    "    with tqdm.tqdm(total=len(lenders)) as progress:\n",
    "        with codecs.open(output_filename, \"w\", \"utf8\") as o:\n",
    "            for lender_index, lender in enumerate(lenders):\n",
    "                for loan_index, score in model.recommend(lender_index, lenders_loans, N=N):\n",
    "                    o.write(\"%s\\t%s\\t%s\\n\" % (lender, loans[loan_index], score))\n",
    "                progress.update(1)\n",
    "    logging.debug(\"generated recommendations in %0.2fs\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_confidence(matrix, alpha=1, weight=None, epsilon=1, dtype='double'):\n",
    "    matrix = matrix.copy()\n",
    "    matrix.data = weight(matrix.data/epsilon) if weight is not None else matrix.data/epsilon\n",
    "    matrix.data = (alpha * matrix.data).astype(dtype)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes users and items unknown to one matrix from the other one\n",
    "def clean_matrices(train, test):\n",
    "    nonzero_rows_train, nonzero_cols_train = train.nonzero()\n",
    "    nonzero_rows_test, nonzero_cols_test = test.nonzero()\n",
    "    \n",
    "    nonzero_rows_train = np.unique(nonzero_rows_train)\n",
    "    nonzero_cols_train = np.unique(nonzero_cols_train)\n",
    "    nonzero_rows_test = np.unique(nonzero_rows_test)\n",
    "    nonzero_cols_test = np.unique(nonzero_cols_test)\n",
    "    \n",
    "    rows_intersection = np.intersect1d(nonzero_rows_train, nonzero_rows_test)\n",
    "    cols_intersection = np.intersect1d(nonzero_cols_train, nonzero_cols_test)\n",
    "\n",
    "    # select only intersecting rows and columns\n",
    "    new_train = train[rows_intersection][:, cols_intersection]\n",
    "    new_test = test[rows_intersection][:, cols_intersection]\n",
    "    \n",
    "    # difference after cleanup\n",
    "    diff_rows_train = train.shape[0] - new_train.shape[0]\n",
    "    diff_cols_train = train.shape[1] - new_train.shape[1]\n",
    "    diff_rows_test = test.shape[0] - new_test.shape[0]\n",
    "    diff_cols_test = test.shape[1] - new_test.shape[1]\n",
    "    \n",
    "    print('Removed %d rows from train set.' % diff_rows_train)\n",
    "    print('Removed %d columns from train set.' % diff_cols_train)\n",
    "    print('Removed %d rows from test set.' % diff_rows_test)\n",
    "    print('Removed %d columns from test set.' % diff_cols_test)\n",
    "    \n",
    "    return (new_train, new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Training model (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(alpha=40, model_name=\"als\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_recommendations(model, \"output.tsv\", N=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section is an usage example on the MovieLens dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, ratings = get_movielens('20m')\n",
    "\n",
    "# remove things < min_rating, and convert to implicit dataset\n",
    "# by considering ratings as a binary preference only\n",
    "ratings.data[ratings.data < 4.0] = 0\n",
    "ratings.eliminate_zeros()\n",
    "ratings.data = np.ones(len(ratings.data))\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (bm25_weight(ratings, B=0.9) * 5).tocsr()\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the train/test samples 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(users, items, ratings, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Implicit testing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies, ratings = get_movielens(\"20m\")\n",
    "# train, test = train_test_split(ratings)\n",
    "\n",
    "coo_mat = utility_matrix.tocoo()\n",
    "train, test = train_test_split(coo_mat)\n",
    "\n",
    "model = AlternatingLeastSquares(use_gpu=False)\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "#map_measure = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision @10: %f' % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc_at_k(model, train_user_items, test_user_items, K=10, show_progress=True):\n",
    "    auc_list = []\n",
    "    lenders_count, loans_count = train_user_items.shape\n",
    "    start = time.time()\n",
    "    \n",
    "    with tqdm.tqdm(total=lenders_count) as progress:\n",
    "        for lender_index in range(lenders_count):\n",
    "            if test_user_items[lender_index, :].nnz == 0:\n",
    "                continue\n",
    "                \n",
    "            lender_row = np.zeros(loans_count)\n",
    "            for loan_index, score in model.recommend(lender_index, train_user_items, N=K):\n",
    "                lender_row[loan_index] = score\n",
    "            \n",
    "            test_lender_row = test_user_items[lender_index, :].toarray().flatten()\n",
    "            roc_auc = roc_auc_score(test_lender_row, lender_row)\n",
    "            auc_list.append(roc_auc)\n",
    "            progress.update(1)\n",
    "            \n",
    "    logging.debug(\"generated mean ROC AUC in %0.2fs\", time.time() - start)\n",
    "    return np.mean(auc_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_prec_auc_at_k(model, train_user_items, test_user_items, K=10, show_progress=True):\n",
    "    auc_list = []\n",
    "    lenders_count, loans_count = train_user_items.shape\n",
    "    start = time.time()\n",
    "    \n",
    "    with tqdm.tqdm(total=lenders_count) as progress:\n",
    "        for lender_index in range(lenders_count):\n",
    "            if test_user_items[lender_index, :].nnz == 0:\n",
    "                continue\n",
    "                \n",
    "            lender_row = np.zeros(loans_count)\n",
    "            for loan_index, score in model.recommend(lender_index, train_user_items, N=K):\n",
    "                lender_row[loan_index] = score\n",
    "            \n",
    "            test_lender_row = test_user_items[lender_index, :].toarray().flatten()\n",
    "            \n",
    "            precision, recall, thresholds = precision_recall_curve(test_lender_row, lender_row, pos_label=1)\n",
    "            prec_auc = auc(recall, precision)                \n",
    "            auc_list.append(prec_auc)\n",
    "            progress.update(1)\n",
    "            \n",
    "    logging.debug(\"generated mean Precision/Recall curve AUC in %0.2fs\", time.time() - start)\n",
    "    return np.mean(auc_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_roc_auc_at_k2(model, train_user_items, test_user_items, K=10, show_progress=True):\n",
    "    auc_list = []\n",
    "    lenders_count, loans_count = train_user_items.shape\n",
    "    start = time.time()\n",
    "    \n",
    "    with tqdm.tqdm(total=lenders_count) as progress:\n",
    "        for lender_index in range(lenders_count):\n",
    "            if test_user_items[lender_index, :].nnz == 0:\n",
    "                continue\n",
    "                \n",
    "            lender_vect = model.user_factors[lender_index]\n",
    "            liked = train_user_items[lender_index].indices\n",
    "\n",
    "            scores = model.item_factors.dot(lender_vect)\n",
    "            scores = np.delete(scores, liked)\n",
    "            \n",
    "            test_lender_row = test_user_items[lender_index, :].toarray().flatten()\n",
    "            test_lender_row = np.delete(test_lender_row, liked)\n",
    "            \n",
    "            roc_auc = roc_auc_score(test_lender_row, scores)\n",
    "            auc_list.append(roc_auc)\n",
    "            progress.update(1)\n",
    "            \n",
    "    logging.debug(\"generated mean ROC AUC in %0.2fs\", time.time() - start)\n",
    "    return np.mean(auc_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coo_mat = utility_matrix.tocoo()\n",
    "train, test = train_test_split(coo_mat)\n",
    "train_user_items = train.T.tocsr()\n",
    "test_user_items = test.T.tocsr()\n",
    "#test_user_items = utility_matrix.T.tocsr()\n",
    "\n",
    "model = AlternatingLeastSquares()\n",
    "model.fit(train)\n",
    "\n",
    "# mean_prec_auc = mean_prec_auc_at_k(model, train_user_items, test_user_items, K=10)\n",
    "# print('Mean Prec AUC score: ', mean_prec_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean ROC AUC 2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_roc_auc = mean_roc_auc_at_k2(model, train_user_items, test_user_items, K=10)\n",
    "print('Mean ROC AUC score: ', mean_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies, ratings = get_movielens(\"1m\")\n",
    "# ratings.data[ratings.data < 4.0] = 0\n",
    "# ratings.eliminate_zeros()\n",
    "# ratings.data = np.ones(len(ratings.data))\n",
    "\n",
    "# train, test = train_test_split(ratings)\n",
    "\n",
    "# model = AlternatingLeastSquares(use_gpu=True)\n",
    "# model.fit(train)\n",
    "\n",
    "# # precision = precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "# # map_measure = mean_average_precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)\n",
    "# mean_roc_auc = mean_roc_auc_at_k2(model, train.T.tocsr(), test.T.tocsr(), K=10)\n",
    "# print('Mean ROC AUC score: ', mean_roc_auc)\n",
    "\n",
    "# # mean_prec_auc = mean_prec_auc_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10)\n",
    "# # print('Mean Precision/Recall curve AUC score: ', mean_prec_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Cross-validation following [this guide](https://www.ethanrosenthal.com/2016/10/19/implicit-mf-part-1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START_DATE = '2000-01-01'\n",
    "# END_DATE = '2015-01-01'\n",
    "\n",
    "# mask = (loans_table['raised_time'] > START_DATE) & (loans_table['raised_time'] <= END_DATE)\n",
    "# plot_data = loans_table.loc[mask]\n",
    "\n",
    "plot_data = loans_table\n",
    "\n",
    "plot_data['raised_time'] = pd.to_datetime(plot_data['raised_time'])\n",
    "plot_data['date_month_year'] = plot_data['raised_time'].dt.to_period(\"M\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "g1 = sns.pointplot(x='date_month_year', y='loan_amount', \n",
    "                   data=plot_data)\n",
    "g1.set_xticklabels(g1.get_xticklabels(),rotation=90)\n",
    "g1.set_title(\"Mean Loan by Month Year\", fontsize=15)\n",
    "g1.set_xlabel(\"\")\n",
    "g1.set_ylabel(\"Loan Amount\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas dataset reading\n",
    "###### (used only for data analysis purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders_table = pd.read_csv('additional-kiva-snapshot/lenders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenders = lenders_table['permanent_name']\n",
    "lenders = lenders.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_lenders_table = pd.read_csv('additional-kiva-snapshot/loans_lenders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_lenders_table.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_size = utility_matrix.shape[0]*utility_matrix.shape[1] # Number of possible interactions in the matrix\n",
    "num_interactions = utility_matrix.nnz # Number of items interacted with\n",
    "sparsity = 100*(1 - (num_interactions/matrix_size))\n",
    "print('Sparsitiy: %f %%' % sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average number of loans per lender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(utility_matrix.tocoo().col, return_counts=True)\n",
    "dict(zip(unique, counts))\n",
    "np.mean(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old testing environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/test splitting script is used from [here](https://gist.github.com/tgsmith61591/ce7d614d7a0442f94cd5ae5d1e51d3c2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collab_split import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_mat = utility_matrix.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = coo_mat.row, coo_mat.col, coo_mat.data\n",
    "users = LabelEncoder().fit_transform(users)\n",
    "items = LabelEncoder().fit_transform(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
